#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Jun 25 16:07:46 2021

@author: jharrison
"""

import numpy as np
import pandas as pd
#Nonsense to force print entirety of a data frame
# pd.options.display.max_columns = None
# pd.options.display.max_rows = None
from functools import partial
from pprint import pprint
from hyperopt import fmin, hp, space_eval, tpe, STATUS_OK, Trials
from hyperopt.pyll import scope, stochastic
# from plotly import express as px
# from plotly import graph_objects as go
# from plotly import offline as pyo
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import make_scorer, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score, KFold, StratifiedShuffleSplit
from sklearn.utils import check_random_state
import joblib
import shap
import re
from csv import writer
import sys

#Pull integer from command line that will correspond to a line in 
#file with otu names to make models for. 
#Doing this way so that I can choose which OTUs to model more easily
#See the script, "choosing_taxa_to_model_with_rf.py"

focal_integer = sys.argv[1] - 1 #because of Pythonic indexing

possibles = pd.read_csv("./processedData/ITS_taxa_to_model_via_randomforest.csv")
focal_taxon = possibles['taxa_its'][focal_integer]

print(focal_taxon)

#feature engineering was done in the neural net program
X = pd.read_csv("./processedData/imputed_scaled_ITS_metadata.csv")

#Bring in taxon proportions, these were wrangled in R after being generated by CNVRG
taxa = pd.read_csv("./processedData/ITSp_estimates_wrangled_for_post_modeling_analysis.csv")
#taxa = pd.read_csv("./processedData/16sp_estimates_wrangled_for_post_modeling_analysis_divided_by_ISD.csv")

#Subset taxa to just sample and the taxon taken from the command line
taxa = taxa[['sample',focal_taxon]]

# Remove diversity from features
#X.columns.values
X = X.loc[:,X.columns != 'shannonsISD']
#Remember that the first 9 fields are not of interest. 

#Need to ensure the samples are in the correct order
#First a bit of wrangling to get a matchable sample name field
#We will merge X and taxa instead of reording one of them
X['compartment'] = X['EN'].map({1: 'EN', 0: 'EP'})
#X['compartment'].value_counts()
X['sample'] = X['plant.x'].str.cat(X['compartment'], sep='_')

X_taxa = pd.merge(X, taxa, on='sample')

#####################
#Do train/test split#
#####################

#Figure out which indices correspond to the samples that had
#the most of the taxon. These top 100 samples will be used for a 
#stratified sampling, since they will include the samples that actually
#had the particular microbe. 

#Determine samples with most of microbe and then make a one hot encoded 
#feature to stratify by
N = 100
res = sorted(range(len(X_taxa[focal_taxon])), key = lambda sub: X_taxa[focal_taxon][sub])[-N:]

X_taxa['focal_taxon_onehot'] = 0
X_taxa.loc[res, 'focal_taxon_onehot'] = 1

#Code from Geron. 

#Handy code for stratified test/train splitting with a random number 
#generator used for reproducibility
#To do a nested stratification of compartment and the presence of focal taxon,
#I am pasting the two together intoa a new feature. As others have said online
#this is a brutal hack. 

X_taxa['brutal_hack'] = X_taxa["EN"] + X_taxa['focal_taxon_onehot']

split = StratifiedShuffleSplit(n_splits=1, test_size=(0.3), random_state=(666))
for train_index, test_index in split.split(X_taxa, X_taxa['brutal_hack']):
    strat_train_set = X_taxa.loc[train_index]
    strat_test_set = X_taxa.loc[test_index]

#define response/predictor split
#First drop various book keeping columns
strat_test_set = strat_test_set.drop(['focal_taxon_onehot', 'brutal_hack', 'sample',
 'region_site',
 'Unnamed: 0',
 'X',
 'taxon.x',
 'region_site_plant',
 'plant.x',
 'forward_barcode',
 'reverse_barcode',
 'locus',
 'samplename'], axis=1)

strat_train_set = strat_train_set.drop(['focal_taxon_onehot', 'brutal_hack', 'sample',
 'region_site',
 'Unnamed: 0',
 'X',
 'taxon.x',
 'region_site_plant',
 'plant.x',
 'forward_barcode',
 'reverse_barcode',
 'locus',
 'samplename'], axis=1)

X_array = np.array(strat_train_set.loc[:, strat_train_set.columns != focal_taxon])
Y_array = np.array(strat_train_set[focal_taxon]).reshape(-1,1)

Xtest_array = np.array(strat_test_set.loc[:, strat_test_set.columns != focal_taxon])
Ytest_array = np.array(strat_test_set[focal_taxon]).reshape(-1,1)

strat_test_set.shape  
strat_train_set.shape

###############################
# hyperparameter optimization #
###############################

#Two options are included here. The first uses
# Bayesian Sequential Model-based Optimization (SMBO) using HyperOpt. 
#The second is a random search with scikit

#Following code inspired by https://www.statestitle.com/resource/visualizing-hyperparameter-optimization-with-hyperopt-and-plotly/

#choose model(s). Useful if multiple learners are used
models = {
   'rf' : RandomForestClassifier
}
# function to define hyper parameter search space. Note that I just have
#one model type here...could add different params for different models using
#elif statements. 
def search_space(model):
    model = model.lower()
    space = {'max_depth': hp.choice('max_depth', range(2,10)),
           'max_features': hp.choice('max_features', range(1,3)),
           'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),
           'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 5, 10]),
           'n_estimators': hp.choice('n_estimators', range(10,100)),
           'criterion': hp.choice('criterion', ["gini", "entropy"])}
    space['model'] = model
    return space

#Define loss function
def get_acc_status(clf,X_,y):
  acc = cross_val_score(clf, X_, y, cv=5).mean() 
  return {'loss': -acc, 'status': STATUS_OK}

#Define objective function. 
#takes hyper parameters and sees what we get. dope or undope?
def obj_fnc(params) : 
   model = params.get('model').lower()
   X_ = scale_normalize(params,X[:]) 
   del params['model']
   clf = models[model](**params) 
   return(get_acc_status(clf,X_,y))

#save the trials and print the best ones
hypopt_trials = Trials()
best_params = fmin(obj_fnc, search_space(model), algo=tpe.suggest, 
max_evals=1000, trials= hypopt_trials)
print(best_params)
print(hypopt_trials.best_trial['result']['loss'])

#Code modified from: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74

# from sklearn.model_selection import RandomizedSearchCV

# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 500, num = 10)]

# # Number of features to consider at every split
# max_features = ['auto', 'sqrt']

# # Maximum number of levels in tree
# max_depth = [int(x) for x in np.linspace(4, 50, num = 2)]
# max_depth.append(None)

# # Minimum number of samples required to split a node
# min_samples_split = [2, 5, 10]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [1, 2, 4]
# # Method of selecting samples for training each tree
# bootstrap = [True, False]
# # Create the random grid
# random_grid = {'n_estimators': n_estimators,
#                 'max_features': max_features,
#                 'max_depth': max_depth,
#                 'min_samples_split': min_samples_split,
#                 'min_samples_leaf': min_samples_leaf,
#                 'bootstrap': bootstrap}

# from pprint import pprint
# pprint(random_grid)

# {'bootstrap': [True, False],
#                 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
#   'max_features': ['auto', 'sqrt'],
#   'min_samples_leaf': [1, 2, 4],
#   'min_samples_split': [2, 5, 10],
#   'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}


# define the model
model = RandomForestRegressor(
                              criterion = 'mse',
                              random_state = 666,
                              n_jobs = -1
                              )
 
# FOR RANDOM SEARCH OF HYPERPARAMETERS
# # Random search of hyper parameters, using 3 fold cross validation, 
# # search across 100 different combinations, and use all available cores
# model_random = RandomizedSearchCV(estimator = model, 
#                                 param_distributions = random_grid, 
#                                 n_iter = 100, 
#                                 cv = 3, 
#                                 verbose=2, 
#                                 random_state=666, 
#                                 n_jobs = -1) #-1 means use all processers


##############
# model eval #
##############

model_random.fit(X=X_array[:,10:], y=Y_array.ravel())

#see what we got
model_random.best_params_

yhat = model_random.predict(Xtest_array[:,10:])

r2 = r2_score(Ytest_array,yhat)
r2

#save the model

XXXXXXXXXXXXXXXXX #make the output file have relevant name

joblib.dump(model_random, './models/randomforest_shannon_16s')

#To load use this syntax
#model_random = joblib.load("PATH")

#Convert the RandomizedSearchCV object to a randomForestRegressor object
tuned_model = RandomForestRegressor(**model_random.best_params_)

#sanity check
# yhat = model_random.predict(Xtest_array[:,10:])
tuned_model.fit(X=X_array[:,10:], y=Y_array.ravel())
yhat2 = tuned_model.predict(Xtest_array[:,10:])
r2 = r2_score(Ytest_array,yhat2)
r2

##############
#SHAP values
############
shap.initjs()

# Create Tree Explainer object that can calculate shap values
explainer = shap.TreeExplainer(tuned_model)

shap_values = explainer.shap_values(X = X_array[:,10:])
list_of_labels = list(strat_train_set.loc[:, strat_train_set.columns != 'shannonsISD'].columns)
feature_names=list_of_labels[10:]

# shap.summary_plot(shap_values, 
#                   X_array[:,10:],
#                   feature_names=feature_names)

# #Extract SHAP values and then remove some features and try rerunning model
shapValues = pd.DataFrame(shap_values, columns=feature_names, 
                          index = strat_train_set.loc[:,'samplename'])

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)


XXXXXXXXXXXXXXXXX #make the output file have relevant name
#write SHAP values to disk. 
shapValues.to_csv(path_or_buf=("./results/shap_values_randomforest_%s"%(i))

###############################################
#Feature removal following SHAP value creation
###############################################

#Figure out unimportant features
duds = shapValues.sum(axis = 0)[abs(shapValues.sum(axis = 0)) < 0.2]

#Here I am searching for taxa in this list of useless features, bc
#I WANT to keep all taxa
l = duds.index.tolist()
r = re.compile(r'.*\s+.*')
newlist = list(filter(r.match, l))
#print(newlist)

#Remove taxa from list of features to remove. Thus keeping taxa.
for i in newlist:
    l.remove(i)

#Now we have a list, l, that has features to be removed. Lets remove them and rerun the model 
###########################################
# Running model again after removing useless features #
###########################################

#Note that the reason to do this is because if one has a bunch of shit features
#they will get picked during feature bagging while training the model, thus
#diluting the effect of the better features. 


XXXXXXXXXXXXXXXXX #All needs redone

X = pd.read_csv("./processedData/imputed_scaled_16S_metadata.csv")

#Remove features that we have deemed not helpful
for i in l:
    X.drop(i, inplace=True, axis=1)

#Do train/test split#
split = StratifiedShuffleSplit(n_splits=1, test_size=(0.2), random_state=(666))
for train_index, test_index in split.split(X, X["EN"]):
    strat_train_set = X.loc[train_index]
    strat_test_set = X.loc[test_index]

strat_test_set.shape  
strat_train_set.shape

X_array = np.array(strat_train_set.loc[:, strat_train_set.columns != 'shannonsISD'])
Y_array = np.array(strat_train_set['shannonsISD']).reshape(-1,1)

Xtest_array = np.array(strat_test_set.loc[:, strat_test_set.columns != 'shannonsISD'])
Ytest_array = np.array(strat_test_set['shannonsISD']).reshape(-1,1)

# refit the model. NOTE that I am not bothering to retune the model as I think
#This goes into diminishing returns

tuned_model.fit(X=X_array[:,10:], y=Y_array.ravel())

yhat = tuned_model.predict(Xtest_array[:,10:])

r2_reduced = r2_score(Ytest_array,yhat)

#Write the taxon and new r2 to the results file

List=['full model',r2,'reduced model', r2_reduced]
  
# Open our existing CSV file in append mode
# Create a file object for this file
with open('./results/randomForest_results_its.csv', 'a') as f_object:
  
    # Pass this file object to csv.writer()
    # and get a writer object
    writer_object = writer(f_object)
  
    # Pass the list as an argument into
    # the writerow()
    writer_object.writerow(List)
  
    #Close the file object
    f_object.close()

