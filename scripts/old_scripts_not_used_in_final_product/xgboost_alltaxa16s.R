rm(list=ls())
library(tidymodels)
library(xgboost)
#library(doParallel)
#all_cores <- parallel::detectCores(logical = FALSE)
#registerDoParallel(cores = all_cores)

set.seed(666)

#Input and minor wrangling

#Usage is predictor matrix, multinomial matrix, integer, taxatomodel (possibles)
#The integer is used to pull a line from a file that has all the taxa to model

inargs <- commandArgs(trailingOnly = TRUE)

X <- read.csv(inargs[1]) #e.g., "./imputed_scaled_ITS_metadata.csv"

#Bring in taxon proportions, these were wrangled in R after being generated by CNVRG
taxa <- read.csv(inargs[2]) #./ITSp_estimates_wrangled_for_post_modeling_analysis_divided_by_ISD.csv
possibles <- read.csv(inargs[4]) # "./processedData/ITS_taxa_to_model_via_randomforest.csv"
focal_taxon <- possibles[inargs[3],]

#Debugging stuff
# taxa <- read.csv("./processedData//ITSp_estimates_wrangled_for_post_modeling_analysis_divided_by_ISD.csv")
#  possibles <- read.csv("./processedData/ITS_taxa_to_model_via_randomforest.csv")
#  focal_taxon <- possibles[1,]
#  X<- read.csv("./processedData/imputed_scaled_ITS_metadata.csv")
X <- X[,names(X) != "shannonsISD"] #remove microbial diversity

#stdout qc
for(i in 1:4){
  print(inargs[i])
}
print(focal_taxon)

X$compartment <- ifelse(X$EN == 1, "EN", NA)
X$compartment[X$EP == 1] <- "EP"
table(X$compartment)

#Make sure our data are in the same order
X$sample <- paste(X$plant.x, X$compartment, sep = "_")
table(X$sample %in% taxa$sample)
#the missing stuff were things I removed due to poor sequencing
merged_dat <- merge(X, taxa, by.x = "sample", by.y = "sample")

#Making a response object for readability
response_taxon <- merged_dat[, names(merged_dat) == focal_taxon]

#Get rid of stuff we don't need in merged_dat (all the bogus stuff was from merging with taxa, so we can index by og dimensions)
merged_dat <- data.frame(response_taxon,
                         merged_dat[,1:length(names(X))])
table(merged_dat$sample %in% taxa$sample)

#train/test split
focal_one_hot <- ifelse(response_taxon > median(response_taxon), 1, 0)
table(focal_one_hot)
merged_dat$stratify <- paste(merged_dat$compartment, focal_one_hot)
merged_dat_split <- rsample::initial_split(
  merged_dat, 
  prop = 0.3, 
  strata = stratify
)

#remove our bookkeeping data as they were causing problems
merged_dat_split$data <- merged_dat_split$data[,13:(length(merged_dat_split[[1]])-1)]
merged_dat_split$data$response_taxon <- response_taxon

#Recipe
phyllo_recipe <- 
  recipe(response_taxon ~ ., data = training(merged_dat_split)) 

#adding some roles
#phyllo_recipe <- recipe(response_taxon ~ ., data = training(merged_dat_split))  %>%
  # update_role(
  #   #Can do via indexing, but am writing names for clarity
  #   #names(merged_dat)[1:11], #stuff we want to recode the role of
  #   sample, 
  #   X.1.x,
  #   region_site,
  #   X.x,
  #   taxon.x,
  #   region_site_plant,
  #   plant.x,
  #   forward_barcode,
  #   reverse_barcode,
  #   locus,
  #   samplename,
  #   stratify, 
  #   # X.1.y, 
  #   new_role = "ID")

summary(phyllo_recipe)

#remove near zero variance predictors
nzv_filter <- phyllo_recipe %>%
  step_nzv(all_predictors())

phyllo_recipe <- prep(nzv_filter)

phyllo_chopped <- bake(
  phyllo_recipe, #the recipe
  new_data = training(merged_dat_split) #these are the data that will be processed
) %>%  
  rsample::vfold_cv(v = 5)

#Define model to be tuned
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
  set_engine("xgboost", objective = "reg:squarederror")

xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )

xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 5
  )

xgboost_wf <- workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(response_taxon ~ .)
# 
# #Look for single level factors and remove
#No longer do this since I added this to the recipe. 

# remover <- function(toexamine) {
#   toremove <- NA
#   for (i in 1:(length(toexamine))) {
#     if (1 %in% names(table(toexamine[, i]))) {
#       next
#     }
#     else{
#       # print("hold up")
#       # print(names(toexamine)[i])
#       # print(table(toexamine[, i]))
#       toremove <- append(toremove, names(toexamine)[i])
#     }
#   }
#   toexamine <- toexamine[,!(names(toexamine) %in% toremove)]
#   return(toexamine)
# }

# #clean up our splits.
# for(i in 1:length(phyllo_chopped$splits)){
#   #This mess...note the -1 so that I don't chop off the response variable
#   out <- remover(phyllo_chopped$splits[[i]]$data[,min(grep("Abies", names(merged_dat))):length(merged_dat)-1 ])
#   phyllo_chopped$splits[[i]]$data <- phyllo_chopped$splits[[i]]$data[,-(min(grep("Abies", names(merged_dat))):length(merged_dat)-1)]
#   phyllo_chopped$splits[[i]]$data <- data.frame(phyllo_chopped$splits[[i]]$data, out)
# }
# 
# #Cut off bogus stuff
# for(i in 1:length(phyllo_chopped$splits)){
#   phyllo_chopped$splits[[i]]$data <- data.frame(phyllo_chopped$splits[[i]]$data[, 12:length(phyllo_chopped$splits[[i]]$data)])
# }

#tune
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = phyllo_chopped,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)

xgboost_best_params <- xgboost_tuned %>%
  tune::select_best("rmse")

#finalize model parameters
xgboost_model_final <- xgboost_model %>% 
  finalize_model(xgboost_best_params)

#finalize workflow
xgboost_wf <- workflows::workflow() %>%
  add_model(xgboost_model_final) %>% 
  add_formula(response_taxon ~ .)

test_processed <- bake(
  phyllo_recipe, 
  new_data = testing(merged_dat_split) 
)

train_processed <- bake(
  phyllo_recipe, 
  new_data = training(merged_dat_split)
)

#Fit and predict
modelfit <- xgboost_model_final %>%
  fit(
    formula = train_processed$response_taxon ~ ., 
    data    = train_processed
  ) %>% predict( new_data = test_processed) %>%
  bind_cols(test_processed)

xgboost_score_test <- 
  modelfit %>%
  yardstick::metrics(response_taxon, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

xgboost_score_test

#Get variable importance
#install.packages("vip")

library(vip)
#This code requires a workflow be passed in.
modelfit <- xgboost_wf %>%
  fit(#formula = train_processed$response_taxon ~ .,
    data = test_processed) %>%
  extract_fit_parsnip() 

save.image(file = paste("./models/XGBoost", focal_taxon, "16s.Rdata", sep = ""))

#This hackish way is a way to get variable importance. Seems clunky
a <- vip(modelfit, geom = "point", num_features = length(names(data.frame(phyllo_chopped$splits[[i]]$data[, 12:length(phyllo_chopped$splits[[i]]$data)])
)))
write.csv(cbind(a$data$Variable,
      a$data$Importance), file = paste("./models/XGBoost", focal_taxon, "vip16s.csv", sep = ""))

